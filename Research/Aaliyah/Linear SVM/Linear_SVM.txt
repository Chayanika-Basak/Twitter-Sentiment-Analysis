Trajectory of work:

1)Working on a text classification problem(Use Case)
2)Refining your training data(Tasks in Hand for the training of ur model)
3)maybe you’ve even experimented with Naive Bayes
4)You’re feeling confident in your dataset
5)Want to take it one step further.(To improve the accuracy perhaps)

Solution:

Enter Support Vector Machines (SVM), a fast and 
dependable classification algorithm that performs 
very well with a limited amount of data to analyze.

Prerequistes:

Perhaps you have dug a bit deeper, and ran into terms like linearly separable, kernel trick and kernel functions. 
Define the terms: 
linearly separable-
kernel trick-
kernel functions-
But fear not! The idea behind the SVM algorithm is simple, and applying it to NLP doesn’t require most of the complicated stuff.

SVM:

A support vector machine (SVM) 
-> is a supervised machine learning model 
-> that uses classification algorithms 
-> for two-group classification problems.

After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.

Advantages:
Compared to newer algorithms like neural networks, they have two main advantages: 
-> higher speed 
-> better performance 
-> with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems, where it’s common to have access to a dataset of at most a couple of thousands of tagged samples.

Example to understand SVM:

Let’s imagine 
-> we have two tags: red and blue
-> and our data has two features: x and y. 
-> We want a classifier that, given a pair of (x,y) coordinates, outputs if it’s either red or blue. 
We plot our already labeled training data on a plane: img1

A support vector machine takes these data points and outputs the hyperplane 
(which in two dimensions it’s simply a line) that best separates the tags. 
This line is the decision boundary: 
anything that falls to one side of it we will classify as blue, 
and anything that falls to the other as red.

But, what exactly is the best hyperplane? For SVM, 
it’s the one that maximizes the margins from both tags. 
In other words: 
the hyperplane (remember it's a line in this case) whose distance to the nearest element of each tag is the largest.

Note: Not all hyperplanes are created equal

reference link: https://www.youtube.com/watch?v=1NxnPkZM9bc

Types of SVM : linear and non linear depending on the dataset in Hand

Now this example was easy, since clearly the data was linearly separable — we could draw a straight line to separate red and blue. 
Sadly, usually things aren’t that simple. Take a look at this case: img2

It’s pretty clear that there’s not a linear decision boundary (a single straight line that separates both tags). 
However, the vectors are very clearly segregated and it looks as though it should be easy to separate them.

So here’s what we’ll do: we will add a third dimension. Up until now we had two dimensions: x and y. 
We create a new z dimension, and we rule that it be calculated a certain way that is convenient for us:
 z = x² + y² (you’ll notice that’s the equation for a circle).

 Check out this 3d visualization to see another example of the same effect: https://youtu.be/3liCbRZPrZA

 For the understanding of the implementation of optimal hyperplane we need to understand the mathematical model which runs in its backend 
 using this we will also be able to hard-code the linear-SVM (refer img3)

Points to note from the above Figure:
a. We can clearly see that SVM tries to maximize the margins and thus called Maximum Margin Classifier.

b. The Support Vectors will have values exactly either {+1, -1}.

c. The more negative the values are for the Green data points the better it is for classification.

d. The more positive the values are for the Red data points the better it is for classification

 How to choose the Correct SVM:

 Suppose we are given 2 Hyperplane one with 100% accuracy (HP1) on the left side and another with >90% accuracy (HP2) on the right side. 
 
 Which one would you think is the correct classifier?

Most of us would pick the HP2 thinking that it because of the maximum margin. But it is the wrong answer.

But Support Vector Machine would choose the HP1 though it has a narrow margin. 
Because though HP2 has maximum margin but it is going against the constrain that: 
each data point must lie on the correct side of the margin and there should be no misclassification. 
This constrain is the hard constrain that Support Vector Machine follows throughout.

This brings us to the discussion about Hard and Soft SVM.
We can now clearly state that HP1 is a Hard SVM(left side) while HP2 is a Soft SVM(right side).
By default, Support Vector Machine implements Hard margin SVM. It works well only if our data is linearly separable.
Hard margin SVM does not allow any misclassification to happen.
In case our data is non-separable/ nonlinear then the Hard margin SVM will not return any hyperplane as it will not be able to separate the data. Hence this is where Soft Margin SVM comes to the rescue.
Soft margin SVM allows some misclassification to happen by relaxing the hard constraints of Support Vector Machine.
Soft margin SVM is implemented with the help of the Regularization parameter (C).

Regularization parameter (C): It tells us how much misclassification we want to avoid.

– Hard margin SVM generally has large values of C.
– Soft margin SVM generally has small values of C.


Relation between Regularization parameter (C) and SVM:

Now that we know what the Regularization parameter (C) does. We need to understand its relation with Support Vector Machine.

Notes::

– As the value of C increases the margin decreases thus Hard SVM.

– If the values of C are very small the margin increases thus Soft SVM.

– Large value of C can cause overfitting therefore we need to select the correct value using Hyperparameter Tuning.

Gamma values : It tells us how much will be the influence of the individual data points on the decision boundary.

– Large Gamma: Fewer data points will influence the decision boundary. 
Therefore, decision boundary becomes non-linear leading to overfitting
– Small Gamma: More data points will influence the decision boundary. 
Therefore, the decision boundary is more generic.