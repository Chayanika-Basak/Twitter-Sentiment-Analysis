***  Start by importing the necessary Python libraries and the dataset:

import pandas as pd
from pandas.api.types import is_numeric_dtype
from sklearn.model_selection import train_test_split
!pip install catboost
import catboost as cb
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns
data = pd.read_csv('https://gist.githubusercontent.com/amankharwal/f4c03b6e26c80d8b526f8f44ce717840/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv')
data.head()



*** 1 -  Data Preparation

data.dropna(subset=['Survived'],inplace=True)
X = data[['Pclass', 'Sex', 'Age', 'Fare']]
y = data['Survived']
X['Pclass'] = X['Pclass'].astype('str')
X['Fare'].fillna(0,inplace=True)
X['Age'].fillna(0,inplace=True)

def get_categorical_indicies(X):
    cats = []
    for col in X.columns:
        if is_numeric_dtype(X[col]):
            pass
        else:
            cats.append(col)
    cat_indicies = []
    for col in cats:
        cat_indicies.append(X.columns.get_loc(col))
    return cat_indicies
categorical_indicies = get_categorical_indicies(X)

def convert_cats(X):
    cats = []
    for col in X.columns:
        if is_numeric_dtype(X[col]):
            pass
        else:
            cats.append(col)
    cat_indicies = []
    for col in cats:
        X[col] = X[col].astype('category')
convert_cats(X)

**  Now Iâ€™m going to split the data into 20% testing and 80% training:

X_train,X_test,y_train,y_test = train_test_split(X, 
                                                 y, 
                                                 test_size=0.2, 
                                                 random_state=101, 
                                                 stratify=y)
                                                 
print('Test Survival Rate:',y_test.sum()/y_test.count())
>> OUTPUT
Test Survival Rate: 0.3854748603351955

train_df = pd.concat([X,y],axis=1)
survived = train_df[train_df['Survived']==1]
deceased = train_df[train_df['Survived']==0]
deceased = deceased.sample(n=len(survived), random_state=101)
train_df = pd.concat([survived,deceased],axis=0)
X_train = train_df.drop('Survived',axis=1)
y_train = train_df['Survived']


***Final Step: Training Model:

train_dataset = cb.Pool(X_train,y_train, 
                        cat_features=categorical_indicies)                                                      
test_dataset = cb.Pool(X_test,y_test,           
                       cat_features=categorical_indicies)
                       
  
model = cb.CatBoostClassifier(loss_function='Logloss',  
                              eval_metric='Accuracy')
grid = {'learning_rate': [0.03, 0.1],
        'depth': [4, 6, 10],
        'l2_leaf_reg': [1, 3, 5,],
        'iterations': [50, 100, 150]}
        
 
model.grid_search(grid,train_dataset)
pred = model.predict(X_test)
print(classification_report(y_test, pred))

>> OUTPUT 

    precision    recall  f1-score   support

           0       0.77      0.89      0.82       110
           1       0.76      0.57      0.65        69

    accuracy                           0.77       179
   macro avg       0.77      0.73      0.74       179
weighted avg       0.77      0.77      0.76       179
