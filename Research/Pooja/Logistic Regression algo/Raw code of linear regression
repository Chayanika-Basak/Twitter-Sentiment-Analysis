Here we will construct Logistic Regression only with Numpy library, the most basic and fundamental one for data analysis in Python.

***  1 - Import dependent libraries, and generate a simple dataset:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
df = pd.DataFrame({“age”: [22,25,47,52, 46,56,55,60,62,61,18,28,27,29,49,55,25,58,19,18,21,26,40,45,50,54,23], “bought_insurance”:[0,0,1,0,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0]})


***  2 - Split dataset into two parts: Training, and Testing:

test = df.sample(7)
train = df[~df.isin(test)]
train.dropna(inplace = True)


***  3 -  Activation function:

def sigmoid(x):
  return 1/(1+np.exp(-x))
  
  
*** 4  -  Loss function:

def square_loss(y_pred, target):
  return np.mean(pow((y_pred — target),2))
  
*** 5 - Split X (feature) part and y (target) part:

X_tr, y_tr = train.age, train[‘bought_insurance’]
X_te, y_te = test.age, test[‘bought_insurance’]

*** 6 -Model setup and running:

lr = 0.01 #learning late
W = np.random.uniform(0,1) # colom 1
b = 0.1
for i in range(10000):
  z = np.dot(X_tr, W) + b
y_pred = sigmoid(z)
l = square_loss(y_pred, y_tr)
gradient_W = np.dot((y_pred-y_tr).T, X_tr)/X_tr.shape[0]
gradient_b = np.mean(y_pred-y_tr)
W = W — lr * gradient_W
b = b — lr* gradient_b

*** 7. Test the performance of the model:

for i in range(len(X_te)):
r = sigmoid(np.dot(X_te, W) + b)


>>>And this will return sequence of probabilities between 0 and 1
>>OUTPUT
[0.7919596733344769, 0.9410453079178125, 0.9152338193767108, 0.08000347093257619, 0.17797780617706674, 0.9045616415408676, 0.8491202536233904]
  

